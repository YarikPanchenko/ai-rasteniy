import os
import shutil
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
from PIL import Image, ImageTk
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms, models
from torch.utils.data import DataLoader, Dataset, random_split
from tqdm import tqdm
import matplotlib.pyplot as plt
from tkinter import simpledialog
import requests
from io import BytesIO

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
CLASSES = ["–û–≥—É—Ä–µ—Ü", "–ü–æ–º–∏–¥–æ—Ä", "–°–∞–ª–∞—Ç", "–ù–µ —Ä–∞—Å—Ç–µ–Ω–∏–µ"]
BATCH_SIZE = 8
MAX_EPOCHS = 30
PATIENCE = 3
VAL_SPLIT = 0.2
IMG_SIZE = 224

# –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
DATASET_DIR = "dataset"
TRAIN_DIR = os.path.join(DATASET_DIR, "train")
VAL_DIR = os.path.join(DATASET_DIR, "val")
TEST_DIR = os.path.join(DATASET_DIR, "test")
RETRAIN_DIR = "retrain_data"
MODEL_PATH = "plant_model.pth"
METRICS_PATH = "training_metrics.png"

# PlantNet API
PLANTNET_API_KEY = ""
PLANTNET_URL = "https://my-api.plantnet.org/v2/identify/all"

# –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫
os.makedirs(TRAIN_DIR, exist_ok=True)
os.makedirs(VAL_DIR, exist_ok=True)
os.makedirs(TEST_DIR, exist_ok=True)
os.makedirs(RETRAIN_DIR, exist_ok=True)
for class_name in CLASSES :
    os.makedirs(os.path.join(TRAIN_DIR, class_name), exist_ok=True)
    os.makedirs(os.path.join(VAL_DIR, class_name), exist_ok=True)
    os.makedirs(os.path.join(TEST_DIR, class_name), exist_ok=True)
    os.makedirs(os.path.join(RETRAIN_DIR, class_name), exist_ok=True)


# –ó–∞–≥—Ä—É–∑–∫–∞/—Å–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
def load_model(pretrained=True) :
    model = models.efficientnet_b0(pretrained=pretrained)
    model.classifier[1] = nn.Linear(1280, len(CLASSES))
    if os.path.exists(MODEL_PATH) :
        model.load_state_dict(torch.load(MODEL_PATH))
    return model


# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
train_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.RandomRotation(15),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
])

val_transform = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(IMG_SIZE),
    transforms.ToTensor(),
])


# Dataset
class PlantDataset(Dataset) :
    def __init__(self, root_dir, transform=None) :
        self.root_dir = root_dir
        self.transform = transform
        self.images = []
        self.labels = []

        for label, class_name in enumerate(CLASSES) :
            class_dir = os.path.join(root_dir, class_name)
            if not os.path.exists(class_dir) :
                continue
            for img_name in os.listdir(class_dir) :
                self.images.append(os.path.join(class_dir, img_name))
                self.labels.append(label)

    def __len__(self) :
        return len(self.images)

    def __getitem__(self, idx) :
        img_path = self.images[idx]
        try :
            image = Image.open(img_path).convert("RGB")
            if self.transform :
                image = self.transform(image)
            return image, self.labels[idx]
        except :
            return None


def collate_fn(batch) :
    batch = list(filter(lambda x : x is not None, batch))
    return torch.utils.data.dataloader.default_collate(batch)


# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
def train_model(initial_train=True) :
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º
    if initial_train :
        train_dir = TRAIN_DIR
        val_dir = VAL_DIR
    else :
        train_dir = RETRAIN_DIR
        val_dir = VAL_DIR  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    if not os.listdir(train_dir) :
        messagebox.showerror("–û—à–∏–±–∫–∞", f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {'–æ–±—É—á–µ–Ω–∏—è' if initial_train else '–¥–æ–æ–±—É—á–µ–Ω–∏—è'}!")
        return

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_dataset = PlantDataset(train_dir, transform=train_transform)
    val_dataset = PlantDataset(val_dir, transform=val_transform)

    # –£–¥–∞–ª—è–µ–º –±–∏—Ç—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    train_dataset.images = [img for img, label in zip(train_dataset.images, train_dataset.labels)
                            if Image.open(img).convert("RGB") is not None]
    train_dataset.labels = [label for img, label in zip(train_dataset.images, train_dataset.labels)
                            if Image.open(img).convert("RGB") is not None]

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0001)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    best_val_loss = float('inf')
    epochs_no_improve = 0
    history = {'train_loss' : [], 'val_loss' : [], 'val_acc' : []}

    progress = tqdm(range(MAX_EPOCHS), desc="–û–±—É—á–µ–Ω–∏–µ" if initial_train else "–î–æ–æ–±—É—á–µ–Ω–∏–µ")
    model.train()

    for epoch in progress :
        # –§–∞–∑–∞ –æ–±—É—á–µ–Ω–∏—è
        train_loss = 0.0
        for images, labels in train_loader :
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)
        history['train_loss'].append(train_loss)

        # –§–∞–∑–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad() :
            for images, labels in val_loader :
                outputs = model(images)
                val_loss += criterion(outputs, labels).item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        val_loss /= len(val_loader)
        accuracy = 100 * correct / total
        history['val_loss'].append(val_loss)
        history['val_acc'].append(accuracy)

        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
        if val_loss < best_val_loss :
            best_val_loss = val_loss
            epochs_no_improve = 0
            torch.save(model.state_dict(), MODEL_PATH)
        else :
            epochs_no_improve += 1
            if epochs_no_improve >= PATIENCE :
                progress.set_postfix({"stop" : f"–≠–ø–æ—Ö–∞ {epoch + 1}"})
                break

        progress.set_postfix({
            "train_loss" : f"{train_loss:.4f}",
            "val_loss" : f"{val_loss:.4f}",
            "val_acc" : f"{accuracy:.2f}%"
        })

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Validation Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history['val_acc'], label='Validation Accuracy', color='green')
    plt.legend()

    plt.savefig(METRICS_PATH)
    plt.close()

    messagebox.showinfo(
        "–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ" if initial_train else "–î–æ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ",
        f"–õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {max(history['val_acc']):.2f}%\n"
        f"–§–∏–Ω–∞–ª—å–Ω—ã–π val_loss: {history['val_loss'][-1]:.4f}"
    )


# GUI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
class PlantClassifierApp :
    def __init__(self, root) :
        self.root = root
        self.root.title("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ä–∞—Å—Ç–µ–Ω–∏–π")
        self.root.geometry("900x850")

        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        self.image_path = None
        self.correct_class = tk.IntVar(value=-1)
        self.current_test_image = None
        self.test_images = []
        self.current_test_index = 0

        # –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
        self.ask_model_loading()

        # –í–∏–¥–∂–µ—Ç—ã
        self.create_widgets()

    def ask_model_loading(self) :
        """–°–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∫–∞–∫ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å"""
        global model

        if not os.path.exists(MODEL_PATH) :
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª–∏ –Ω–µ—Ç, —Å—Ä–∞–∑—É –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            model = load_model(pretrained=True)
            messagebox.showinfo("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", "–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –æ–±—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏.")
            train_model(initial_train=True)
            return

        answer = messagebox.askyesnocancel(
            "–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏",
            "–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å. –•–æ—Ç–∏—Ç–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å –µ—ë?\n"
            "–î–∞ - –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å\n"
            "–ù–µ—Ç - –æ–±—É—á–∏—Ç—å –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å\n"
            "–û—Ç–º–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è"
        )

        if answer is None :  # –û—Ç–º–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
            model = load_model(pretrained=True)
            messagebox.showinfo("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (–±–µ–∑ –¥–æ–æ–±—É—á–µ–Ω–∏—è)")
        elif answer :  # –î–∞ - –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            model = load_model(pretrained=False)
            messagebox.showinfo("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", "–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        else :  # –ù–µ—Ç - –æ–±—É—á–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
            model = load_model(pretrained=True)
            train_model(initial_train=True)

    def create_widgets(self) :
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        tk.Label(self.root, text="–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ä–∞—Å—Ç–µ–Ω–∏–π", font=("Arial", 16, "bold")).pack(pady=10)

        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        self.create_dataset_section()

        # 2. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.create_training_section()

        # 3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.create_testing_section()

        # 4. –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.create_results_section()

    def create_dataset_section(self) :
        frame = tk.LabelFrame(self.root, text="1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö", padx=10, pady=10)
        frame.pack(fill="x", padx=10, pady=5)

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–ª–∞—Å—Å–∞–º
        tk.Label(frame, text="–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:", anchor="w").pack(fill="x")

        class_btn_frame = tk.Frame(frame)
        class_btn_frame.pack(fill="x", pady=5)

        for i, class_name in enumerate(CLASSES) :
            tk.Button(
                class_btn_frame,
                text=f"–ó–∞–≥—Ä—É–∑–∏—Ç—å {class_name}",
                command=lambda cn=class_name : self.load_class_images(cn, "train")
            ).pack(side="left", padx=5, expand=True)

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        tk.Label(frame, text="–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ:", anchor="w").pack(fill="x", pady=(10, 0))
        tk.Button(frame, text="–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", command=self.load_test_images).pack(pady=5)

    def plantnet_identify(self, image_path) :
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞—Å—Ç–µ–Ω–∏—è —á–µ—Ä–µ–∑ PlantNet API"""
        try :
            with open(image_path, 'rb') as img_file :
                img_data = img_file.read()

            files = {
                'images' : ('image.jpg', BytesIO(img_data)),
                'organs' : (None, 'leaf')  # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ 'flower', 'fruit' –∏ —Ç.–¥.
            }

            params = {
                'api-key' : PLANTNET_API_KEY,
                'include-related-images' : 'true',
                'no-reject' : 'false',
                'lang' : 'ru'
            }

            response = requests.post(PLANTNET_URL, files=files, params=params)
            response.raise_for_status()
            data = response.json()

            if 'results' in data and data['results'] :
                return data
            return None

        except Exception as e :
            print(f"PlantNet API error: {e}")
            return None

    def create_training_section(self) :
        frame = tk.LabelFrame(self.root, text="2. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏", padx=10, pady=10)
        frame.pack(fill="x", padx=10, pady=5)

        btn_frame = tk.Frame(frame)
        btn_frame.pack(fill="x", pady=5)

        tk.Button(btn_frame, text="–û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å",
                  command=lambda : train_model(initial_train=True)).pack(side="left", padx=5)
        tk.Button(btn_frame, text="–î–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å",
                  command=lambda : train_model(initial_train=False)).pack(side="left", padx=5)

    def create_testing_section(self) :
        frame = tk.LabelFrame(self.root, text="3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏", padx=10, pady=10)
        frame.pack(fill="x", padx=10, pady=5)

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        self.canvas = tk.Canvas(frame, width=IMG_SIZE, height=IMG_SIZE, bg="gray")
        self.canvas.pack(pady=5)

        # –ù–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ —Ç–µ—Å—Ç–æ–≤—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        nav_frame = tk.Frame(frame)
        nav_frame.pack(fill="x", pady=5)

        self.btn_prev = tk.Button(nav_frame, text="< –ù–∞–∑–∞–¥", command=self.prev_test_image, state=tk.DISABLED)
        self.btn_prev.pack(side="left", padx=5)

        self.btn_next = tk.Button(nav_frame, text="–í–ø–µ—Ä–µ–¥ >", command=self.next_test_image, state=tk.DISABLED)
        self.btn_next.pack(side="left", padx=5)

        self.btn_predict = tk.Button(nav_frame, text="–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å", command=self.run_prediction, state=tk.DISABLED)
        self.btn_predict.pack(side="left", padx=5)

        # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        self.result_label = tk.Label(frame, text="", font=("Arial", 12))
        self.result_label.pack(pady=5)

        # –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å
        feedback_frame = tk.Frame(frame)
        feedback_frame.pack(fill="x", pady=5)

        tk.Label(feedback_frame, text="–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –∫–ª–∞—Å—Å:").pack(side="left")

        for i, class_name in enumerate(CLASSES) :
            tk.Radiobutton(
                feedback_frame,
                text=class_name,
                variable=self.correct_class,
                value=i
            ).pack(side="left", padx=5)

        tk.Button(frame, text="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø—Ä–∞–≤–∫—É", command=self.save_feedback).pack(pady=5)

    def create_results_section(self) :
        frame = tk.LabelFrame(self.root, text="4. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã", padx=10, pady=10)
        frame.pack(fill="x", padx=10, pady=5)

        self.metrics_label = tk.Label(frame, text="–ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –ø–æ—è–≤—è—Ç—Å—è –∑–¥–µ—Å—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è")
        self.metrics_label.pack(pady=5)

        btn_frame = tk.Frame(frame)
        btn_frame.pack(fill="x", pady=5)

        tk.Button(btn_frame, text="–ü–æ–∫–∞–∑–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏", command=self.show_metrics).pack(side="left", padx=5)
        tk.Button(btn_frame, text="–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å", command=self.save_model).pack(side="left", padx=5)

    def load_class_images(self, class_name, dataset_type="train") :
        target_dir = os.path.join(DATASET_DIR, dataset_type, class_name)

        # –û—á–∏—â–∞–µ–º —Ü–µ–ª–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        if os.path.exists(target_dir) :
            shutil.rmtree(target_dir)
        os.makedirs(target_dir)

        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –¥–∏–∞–ª–æ–≥ –≤—ã–±–æ—Ä–∞ —Ñ–∞–π–ª–æ–≤
        filepaths = filedialog.askopenfilenames(
            title=f"–í—ã–±–µ—Ä–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {class_name}",
            filetypes=[("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "*.jpg *.jpeg *.png *.bmp")]
        )

        if not filepaths :
            return

        # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        count = 0
        for filepath in filepaths :
            try :
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª —è–≤–ª—è–µ—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
                with Image.open(filepath) as img :
                    img.verify()

                # –ö–æ–ø–∏—Ä—É–µ–º –≤ —Ü–µ–ª–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                filename = os.path.basename(filepath)
                shutil.copy(filepath, os.path.join(target_dir, filename))
                count += 1
            except :
                continue

        messagebox.showinfo("–£—Å–ø–µ—Ö", f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–ª–∞—Å—Å–∞ {class_name}")

    def load_test_images(self) :
        # –û—á–∏—â–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        test_all_dir = os.path.join(TEST_DIR, "all")
        if os.path.exists(test_all_dir) :
            shutil.rmtree(test_all_dir)
        os.makedirs(test_all_dir)

        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –¥–∏–∞–ª–æ–≥ –≤—ã–±–æ—Ä–∞ —Ñ–∞–π–ª–æ–≤
        filepaths = filedialog.askopenfilenames(
            title="–í—ã–±–µ—Ä–∏—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è",
            filetypes=[("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è", "*.jpg *.jpeg *.png *.bmp")]
        )

        if not filepaths :
            return

        # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        self.test_images = []
        count = 0
        for filepath in filepaths :
            try :
                with Image.open(filepath) as img :
                    img.verify()

                filename = os.path.basename(filepath)
                dest_path = os.path.join(test_all_dir, filename)
                shutil.copy(filepath, dest_path)
                self.test_images.append(dest_path)
                count += 1
            except :
                continue

        if count > 0 :
            self.current_test_index = 0
            self.show_test_image(0)
            self.btn_next.config(state=tk.NORMAL if len(self.test_images) > 1 else tk.DISABLED)
            self.btn_prev.config(state=tk.DISABLED)
            self.btn_predict.config(state=tk.NORMAL)
            messagebox.showinfo("–£—Å–ø–µ—Ö", f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {count} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        else :
            messagebox.showwarning("–û—à–∏–±–∫–∞", "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

    def show_test_image(self, index) :
        if 0 <= index < len(self.test_images) :
            self.current_test_index = index
            self.current_test_image = self.test_images[index]

            image = Image.open(self.current_test_image)
            image.thumbnail((IMG_SIZE, IMG_SIZE))
            self.photo = ImageTk.PhotoImage(image)
            self.canvas.create_image(0, 0, anchor=tk.NW, image=self.photo)

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–Ω–æ–ø–æ–∫ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
            self.btn_prev.config(state=tk.NORMAL if index > 0 else tk.DISABLED)
            self.btn_next.config(state=tk.NORMAL if index < len(self.test_images) - 1 else tk.DISABLED)

            # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.result_label.config(text="")
            self.correct_class.set(-1)

    def prev_test_image(self) :
        self.show_test_image(self.current_test_index - 1)

    def next_test_image(self) :
        self.show_test_image(self.current_test_index + 1)

    def run_prediction(self) :
        if not self.current_test_image :
            return

        try :
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞—à–µ–π –º–æ–¥–µ–ª—å—é
            image = Image.open(self.current_test_image).convert("RGB")
            image_tensor = val_transform(image).unsqueeze(0)

            with torch.no_grad() :
                outputs = model(image_tensor)
                probabilities = torch.nn.functional.softmax(outputs, dim=1)
                confidence, predicted_class = torch.max(probabilities, 1)
                confidence = confidence.item()
                predicted_class = predicted_class.item()

            result_text = ""

            # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è - –∏—Å–ø–æ–ª—å–∑—É–µ–º PlantNet
            if confidence < 0.7 :
                result_text += "üîç –ù–∞—à–∞ –º–æ–¥–µ–ª—å –Ω–µ —É–≤–µ—Ä–µ–Ω–∞. –ó–∞–ø—Ä–æ—Å –∫ PlantNet...\n"

                # –í—ã–∑—ã–≤–∞–µ–º PlantNet API
                plantnet_data = self.plantnet_identify(self.current_test_image)

                if plantnet_data :
                    best_match = plantnet_data['results'][0]
                    scientific_name = best_match['species']['scientificNameWithoutAuthor']
                    common_name = best_match['species'].get('commonNames', ['–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'])[0]
                    score = best_match['score']

                    result_text += (
                        f"\nüåø PlantNet –æ–ø—Ä–µ–¥–µ–ª–∏–ª —Ä–∞—Å—Ç–µ–Ω–∏–µ –∫–∞–∫:\n"
                        f"–ù–∞—É—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: {scientific_name}\n"
                        f"–û–±—â–µ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: {common_name}\n"
                        f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {score * 100:.1f}%\n"
                    )

                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    if 'images' in best_match :
                        result_text += "\n–ü–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n"
                        for i, img in enumerate(best_match['images'][:3], 1) :
                            result_text += f"{i}. {img['url']}\n"
                else :
                    result_text += "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞—Å—Ç–µ–Ω–∏–µ —á–µ—Ä–µ–∑ PlantNet\n"

                # –í—Å–µ —Ä–∞–≤–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏
                result_text += "\n–ù–∞—à–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:\n"

            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏
            result_text += f"{CLASSES[predicted_class]}: {confidence * 100:.1f}%\n"

            # –¢–æ–ø-3 –≤–∞—Ä–∏–∞–Ω—Ç–∞
            top_probs, top_classes = torch.topk(probabilities, 3)
            for i in range(top_probs.size(1)) :
                result_text += f"{CLASSES[top_classes[0][i]]}: {top_probs[0][i] * 100:.1f}%\n"

            self.result_label.config(text=result_text)

        except Exception as e :
            messagebox.showerror("–û—à–∏–±–∫–∞", f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {str(e)}")

    def save_feedback(self) :
        if not self.current_test_image or self.correct_class.get() == -1 :
            messagebox.showerror("–û—à–∏–±–∫–∞", "–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –≤—ã–±–µ—Ä–∏—Ç–µ –∫–ª–∞—Å—Å!")
            return

        correct_class_idx = self.correct_class.get()
        class_name = CLASSES[correct_class_idx]
        class_dir = os.path.join(RETRAIN_DIR, class_name)
        os.makedirs(class_dir, exist_ok=True)

        img_name = os.path.basename(self.current_test_image)
        save_path = os.path.join(class_dir, img_name)
        shutil.copy(self.current_test_image, save_path)

        messagebox.showinfo("–£—Å–ø–µ—Ö", f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∫–∞–∫ {class_name} –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è!")
        self.correct_class.set(-1)

    def show_metrics(self) :
        if os.path.exists(METRICS_PATH) :
            image = Image.open(METRICS_PATH)
            image.show()
        else :
            messagebox.showwarning("–í–Ω–∏–º–∞–Ω–∏–µ", "–ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")

    def save_model(self) :
        torch.save(model.state_dict(), MODEL_PATH)
        messagebox.showinfo("–£—Å–ø–µ—Ö", f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}")


# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
if __name__ == "__main__" :
    root = tk.Tk()
    app = PlantClassifierApp(root)
    root.mainloop()